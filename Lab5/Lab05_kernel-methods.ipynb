{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab05_kernel-methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xl_-W_aXqjJ2",
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "source": [
        "# Lab 5 - Kernel Methods\n",
        "\n",
        "### Author: Szymon Nowakowski\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B1K2cKMGKi0Z",
      "metadata": {
        "id": "B1K2cKMGKi0Z"
      },
      "source": [
        "# Introduction to Kernel Methods  \n",
        "-----------------------  \n",
        "\n",
        "Kernel methods are a class of machine learning techniques that enable models to capture complex relationships in data by mapping inputs into high-dimensional feature spaces. Instead of working directly with raw features, kernel methods rely on kernel functions that compute inner products in these transformed spaces, allowing models to discover nonlinear patterns while avoiding explicit feature transformations.  \n",
        "\n",
        "The fundamental idea behind kernel methods is to leverage the **kernel trick**, which replaces explicit high-dimensional computations with efficient similarity measurements. This approach is particularly useful in algorithms like **Support Vector Machines (SVMs)**, **kernel regression**, and **Gaussian processes**, where the goal is to find decision boundaries or make predictions in feature spaces that may be infeasible to compute directly.  \n",
        "\n",
        "Kernel methods are crucial for problems where linear models fail to capture the underlying structure, enabling flexible, nonlinear decision functions without excessive computational cost. They also provide a principled framework for incorporating domain knowledge through custom kernel functions, making them widely applicable across diverse fields such as bioinformatics, computer vision, and natural language processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VcUdd4FuOi96",
      "metadata": {
        "id": "VcUdd4FuOi96"
      },
      "source": [
        "# Support Vector Machines (SVMs) and Margin Boundaries for Separable Case\n",
        "------------------------\n",
        "\n",
        "In **Support Vector Machines (SVMs)**, we want to find the **maximum margin separator**, which is a hyperplane that separates two classes while maximizing the margin. The **decision boundary** is given by:\n",
        "\n",
        "$$ w^\\top x + b = 0 $$\n",
        "\n",
        "However, the margin itself is defined by **two additional hyperplanes** that run parallel to the decision boundary. These **margin boundaries** are:\n",
        "\n",
        "$$ w^\\top x + b = +1 \\quad \\text{(for the positive class)} $$  \n",
        "$$ w^\\top x + b = -1 \\quad \\text{(for the negative class)} $$  \n",
        "\n",
        "These specific values $ \\pm 1 $ come from the **normalization condition** we impose during the SVM optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wO9Vwk3fOtsd",
      "metadata": {
        "id": "wO9Vwk3fOtsd"
      },
      "source": [
        "\n",
        "\n",
        "## 1. The Role of the Margin Boundaries\n",
        "\n",
        "We want to ensure that all correctly classified points satisfy:\n",
        "\n",
        "$$ y_i (w^\\top x_i + b) \\geq 1, \\quad \\forall i. $$\n",
        "\n",
        "This means:\n",
        "\n",
        "- If $ y_i = +1 $ (positive class), we require:\n",
        "\n",
        "  $$ w^\\top x_i + b \\geq 1. $$\n",
        "\n",
        "- If $ y_i = -1 $ (negative class), we require:\n",
        "\n",
        "  $$ w^\\top x_i + b \\leq -1. $$\n",
        "\n",
        "These two conditions define **two parallel hyperplanes** that **contain the closest data points** (support vectors).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Da8kyQOXW5RZ",
      "metadata": {
        "id": "Da8kyQOXW5RZ"
      },
      "source": [
        "## Figure 12.1 (left) from *The Elements of Statistical Learning*\n",
        "\n",
        "We shall reproduce Figure 12.1 (left - separable case) from *The Elements of Statistical Learning*\n",
        "\n",
        "📚 **See also** → *The Elements of Statistical Learning*:  \n",
        "Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer, 2009.  \n",
        "[📖 Link to the book](https://hastie.su.domains/ElemStatLearn/)\n",
        "(Chapter 12, Figure 12.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ssQZ3bkNOlhC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "ssQZ3bkNOlhC",
        "outputId": "e495afc3-f7f1-4bfd-890d-b20953571711"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/home/konstanty/.pyenv/versions/3.12.3/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 1. Generate linearly separable data\n",
        "np.random.seed(42)\n",
        "X = np.r_[\n",
        "    np.random.randn(20, 2) - [2, 2],\n",
        "    np.random.randn(20, 2) + [2, 2]\n",
        "]\n",
        "y = np.array([0]*20 + [1]*20)\n",
        "\n",
        "# 2. Train a linear SVM classifier (clf)\n",
        "clf = SVC(kernel='linear', C=1e5)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Extract parameters w and b from the trained model\n",
        "w = clf.coef_[0]      # [w1, w2]\n",
        "b = clf.intercept_[0] # scalar bias\n",
        "\n",
        "# 3. Create a grid of x-values for plotting the decision boundary and margins\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "x_plot = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "# Decision boundary: w^T x + b = 0 ==> x2 = -(w1*x1 + b)/w2\n",
        "def line_y(x, offset=0.0):\n",
        "    return -(w[0]*x + (b - offset)) / w[1]\n",
        "\n",
        "y_decision = line_y(x_plot, offset=0)\n",
        "y_margin_plus = line_y(x_plot, offset=1)   # w^T x + b = 1\n",
        "y_margin_minus = line_y(x_plot, offset=-1) # w^T x + b = -1\n",
        "\n",
        "# 4. Plot the data points\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k', s=70)\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(X[:, 1].min()-1, X[:, 1].max()+1)\n",
        "plt.title('SVM Decision Boundary and Margins')\n",
        "\n",
        "# Plot the decision boundary (solid line)\n",
        "plt.plot(x_plot, y_decision, 'k-')\n",
        "# Plot the margins (dashed lines)\n",
        "plt.plot(x_plot, y_margin_plus, 'k--')\n",
        "plt.plot(x_plot, y_margin_minus, 'k--')\n",
        "\n",
        "# 5. Label the lines directly on the plot (instead of using a legend)\n",
        "# Choose some x-value near the right side for placing text\n",
        "x_text = x_max - 0.5\n",
        "# Decision boundary label\n",
        "y_text_decision = line_y(x_text, offset=0)\n",
        "plt.text(x_text, y_text_decision + 0.3, r'$w^T x + b = 0$', fontsize=12)\n",
        "\n",
        "# +1 margin label\n",
        "y_text_plus = line_y(x_text, offset=1)\n",
        "plt.text(x_text, y_text_plus + 0.3, r'$w^T x + b = 1$', fontsize=12)\n",
        "\n",
        "# -1 margin label\n",
        "y_text_plus = line_y(x_text, offset=-1)\n",
        "plt.text(x_text, y_text_plus + 0.3, r'$w^T x + b = -1$', fontsize=12)\n",
        "\n",
        "# 6. Draw and label the orthogonal distance 1/||w||\n",
        "# Pick a point on the decision boundary in the middle of the plot\n",
        "x_center = (x_min + x_max) / 2\n",
        "y_center = line_y(x_center, offset=0)  # on the boundary\n",
        "center_point = np.array([x_center, y_center])\n",
        "\n",
        "# The direction normal to the boundary is parallel to w\n",
        "w_norm = np.linalg.norm(w)\n",
        "direction = w / w_norm\n",
        "\n",
        "# One endpoint on the decision boundary, the other on the margin w^T x + b = 1\n",
        "# The distance between them along w is 1 / ||w||\n",
        "margin_point = center_point + (1 / w_norm) * direction\n",
        "\n",
        "# Plot the orthogonal line segment\n",
        "plt.plot([center_point[0], margin_point[0]],\n",
        "         [center_point[1], margin_point[1]], 'r-', lw=2)\n",
        "\n",
        "# Label the distance 1/||w|| near the midpoint of that segment\n",
        "mid_point = (center_point + margin_point) / 2\n",
        "plt.text(mid_point[0] + 0.1, mid_point[1], r'$\\frac{1}{\\|w\\|}$', color='r', fontsize=12)\n",
        "\n",
        "\n",
        "plt.gca().set_aspect('equal')   #make the plot exactly square to keep orthogonal lines - well, orthogonal\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31pzycPFT0l9",
      "metadata": {
        "id": "31pzycPFT0l9"
      },
      "source": [
        "## The Margin\n",
        "\n",
        "The margin size is the distance between the margin boundaries (which are represented by the hyperplanes $ w^T x + b = \\pm 1 $).\n",
        "The distance between these two hyperplanes is given by:\n",
        "\n",
        "$$ \\frac{2}{\\|w\\|} $$\n",
        "\n",
        "This result comes from the fact that the perpendicular distance from a point $x$ to the decision boundary is:\n",
        "\n",
        "$$ \\frac{|w^T x + b|}{\\|w\\|} $$\n",
        "\n",
        "## Minimization Problem\n",
        "\n",
        "Maximizing the margin,\n",
        "\n",
        "$$ \\frac{2}{\\|w\\|} $$\n",
        "\n",
        "is equivalent to minimizing $\\|w\\|$, and for mathematical convenience, it is standard to minimize:\n",
        "\n",
        "$$ \\frac{1}{2}\\|w\\|^2 $$\n",
        "\n",
        "This leads to the following constrained optimization problem:\n",
        "\n",
        "$$ \\min_{w, b} \\frac{1}{2}\\|w\\|^2 $$\n",
        "\n",
        "subject to the constraints:\n",
        "\n",
        "$$ y_i (w^T x_i + b) \\geq 1, \\quad \\forall i $$\n",
        "\n",
        "where $y_i \\in \\{-1, 1\\}$ are the class labels. This formulation ensures that all data points are correctly classified and lie outside or exactly on the margin boundaries while maximizing the margin size.\n",
        "\n",
        "This is exactly the formulation of the problem solved in a\n",
        "`sklearn.svm.SVC` call in the code above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PoJnqUyRZ6OR",
      "metadata": {
        "id": "PoJnqUyRZ6OR"
      },
      "source": [
        "# Support Vector Machines (SVMs) and Margin Boundaries for Non-Separable Case\n",
        "\n",
        "------------------------\n",
        "\n",
        "In **Support Vector Machines (SVMs)**, the non-separable case occurs when the data cannot be perfectly separated by a linear hyperplane. To handle this, **slack variables** (pl. *zmienne pomocnicze w kontekście SVM z miękkim marginesem, zmienne luzu*) are introduced, allowing some misclassifications while still aiming to find the best possible hyperplane that maximizes the margin.\n",
        "\n",
        "The decision boundary is still given by:\n",
        "\n",
        "$$ w^\\top x + b = 0 $$\n",
        "\n",
        "However, we now introduce **slack variables** $ \\xi_i \\geq 0 $ for each data point to allow some points to lie inside the margin or on the wrong side of the hyperplane."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IASRe7agZ9Df",
      "metadata": {
        "id": "IASRe7agZ9Df"
      },
      "source": [
        "\n",
        "\n",
        "## The Role of Slack Variables\n",
        "\n",
        "In the non-separable case, the constraints become:\n",
        "\n",
        "$$ y_i (w^\\top x_i + b) \\geq 1 - \\xi_i, \\quad \\forall i. $$\n",
        "\n",
        "This means:\n",
        "\n",
        "- If $ \\xi_i = 0 $, the point is correctly classified and lies outside or exactly on the margin boundary.\n",
        "- If $ 0 < \\xi_i < 1 $, the point lies inside the margin but is still correctly classified.\n",
        "- If $ \\xi_i \\geq 1 $, the point is misclassified.\n",
        "\n",
        "The goal is to balance maximizing the margin and minimizing the total slack."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xHTcS0oRujuV",
      "metadata": {
        "id": "xHTcS0oRujuV"
      },
      "source": [
        "\n",
        "\n",
        "## The Optimization Problem for the Soft Margin SVM\n",
        "\n",
        "This approach is often referred to as a **Soft Margin SVM** (pl. *SVM z miękkim marginesem*), as it allows the decision boundary to be flexible, tolerating some violations of the margin constraints for better generalization.\n",
        "\n",
        "The SVM optimization problem for the non-separable case is:\n",
        "\n",
        "$$ \\min_{w, b, \\xi} \\left( \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i \\right) $$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$ y_i (w^\\top x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i. $$\n",
        "\n",
        "Here, $ C > 0 $ is a **regularization parameter** that controls the trade-off between maximizing the margin and minimizing the misclassification error:\n",
        "\n",
        "- A **large $C$** emphasizes minimizing slack variables, leading to fewer misclassifications but possibly a smaller margin.\n",
        "- A **small $C$** allows more misclassifications, potentially leading to a larger margin.\n",
        "\n",
        "\n",
        "The final decision function remains:\n",
        "\n",
        "$$ f(x) = \\text{sign}(w^\\top x + b) $$\n",
        "\n",
        "but it now reflects the balance between margin size and classification accuracy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6EnFerX0zAy6",
      "metadata": {
        "id": "6EnFerX0zAy6"
      },
      "source": [
        "## Figure 12.1 (right) from *The Elements of Statistical Learning*\n",
        "\n",
        "We shall reproduce Figure 12.1 (right - non-separable case) from *The Elements of Statistical Learning*\n",
        "\n",
        "📚 **See also** → *The Elements of Statistical Learning*:  \n",
        "Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer, 2009.  \n",
        "[📖 Link to the book](https://hastie.su.domains/ElemStatLearn/)\n",
        "(Chapter 12, Figure 12.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fPPFXsafxw8I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "fPPFXsafxw8I",
        "outputId": "849092d8-9f2d-40b4-d7ea-630a64e4abdd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 1. Generate linearly separable data with 3 non-separable points\n",
        "np.random.seed(42)\n",
        "X = np.r_[\n",
        "    np.random.randn(20, 2) - [2, 2],\n",
        "    np.random.randn(20, 2) + [2, 2]\n",
        "]\n",
        "y = np.array([0]*20 + [1]*20)\n",
        "\n",
        "# Add 3 non-separable points.\n",
        "X = np.vstack([X, [[1, 0], [1, -1], [1, 1]]])\n",
        "y = np.hstack([y, [0, 1, 0]])  # Assign classes\n",
        "\n",
        "# 2. Train a linear SVM classifier (clf)\n",
        "clf = SVC(kernel='linear', C=1e5)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Extract parameters w and b from the trained model\n",
        "w = clf.coef_[0]      # [w1, w2]\n",
        "b = clf.intercept_[0] # scalar bias\n",
        "\n",
        "# 3. Create a grid of x-values for plotting the decision boundary and margins\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "x_plot = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "# Decision boundary: w^T x + b = 0 ==> x2 = -(w1*x1 + b)/w2\n",
        "def line_y(x, offset=0.0):\n",
        "    return -(w[0]*x + (b - offset)) / w[1]\n",
        "\n",
        "y_decision = line_y(x_plot, offset=0)\n",
        "y_margin_plus = line_y(x_plot, offset=1)   # w^T x + b = 1\n",
        "y_margin_minus = line_y(x_plot, offset=-1) # w^T x + b = -1\n",
        "\n",
        "# 4. Plot the data points\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k', s=70)\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(X[:, 1].min()-1, X[:, 1].max()+1)\n",
        "plt.title('SVM with Slack Variables')\n",
        "\n",
        "# Plot the decision boundary (solid line)\n",
        "plt.plot(x_plot, y_decision, 'k-')\n",
        "# Plot the margins (dashed lines)\n",
        "plt.plot(x_plot, y_margin_plus, 'k--')\n",
        "plt.plot(x_plot, y_margin_minus, 'k--')\n",
        "\n",
        "# Label the decision boundary and margins\n",
        "x_text = x_max - 0.5\n",
        "y_text_decision = line_y(x_text, offset=0)\n",
        "plt.text(x_text, y_text_decision + 0.3, r'$w^T x + b = 0$', fontsize=12)\n",
        "\n",
        "y_text_plus = line_y(x_text, offset=1)\n",
        "plt.text(x_text, y_text_plus + 0.3, r'$w^T x + b = 1$', fontsize=12)\n",
        "\n",
        "y_text_minus = line_y(x_text, offset=-1)\n",
        "plt.text(x_text, y_text_minus + 0.3, r'$w^T x + b = -1$', fontsize=12)\n",
        "\n",
        "# 5. Mark non-separable points with arrows and labels\n",
        "non_separable_points = X[-3:]\n",
        "labels = [r'$\\xi_1$', r'$\\xi_2$', r'$\\xi_3$']\n",
        "\n",
        "for i, (point, label) in enumerate(zip(non_separable_points, labels)):\n",
        "    # Calculate projection onto margin\n",
        "    margin_offset = 1 if y[-3:][i] == 1 else -1\n",
        "    margin_point = point - ((w @ point + b - margin_offset) / np.dot(w, w)) * w\n",
        "\n",
        "    # Draw arrow\n",
        "    plt.arrow(margin_point[0], margin_point[1],\n",
        "              point[0] - margin_point[0], point[1] - margin_point[1],\n",
        "              head_width=0.1, head_length=0.1, fc='green', ec='green')\n",
        "\n",
        "    # Label the slack variable\n",
        "    mid_x = (point[0] + margin_point[0]) / 2\n",
        "    mid_y = (point[1] + margin_point[1]) / 2\n",
        "    plt.text(mid_x, mid_y, label, fontsize=12, color='green')\n",
        "\n",
        "# 6. Draw and label the orthogonal distance 1/||w||\n",
        "x_center = (x_min + x_max) / 2\n",
        "y_center = line_y(x_center, offset=0)\n",
        "center_point = np.array([x_center, y_center])\n",
        "\n",
        "w_norm = np.linalg.norm(w)\n",
        "direction = w / w_norm\n",
        "\n",
        "margin_point = center_point + (1 / w_norm) * direction\n",
        "\n",
        "plt.plot([center_point[0], margin_point[0]],\n",
        "         [center_point[1], margin_point[1]], 'r-', lw=2)\n",
        "\n",
        "mid_point = (center_point + margin_point) / 2\n",
        "plt.text(mid_point[0] + 0.1, mid_point[1], r'$\\frac{1}{\\|w\\|}$', color='r', fontsize=12)\n",
        "\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aanv6YW70y3q",
      "metadata": {
        "id": "aanv6YW70y3q"
      },
      "source": [
        "## Task\n",
        "\n",
        "Explain: why, after we add the three candidate non-separable points in the code below (by uncommenting the two commented lines) we get a surprising, well separated chart. Shouldn't the $\\xi$ variables be introduced, instead?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GnoScYUBzMTK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "GnoScYUBzMTK",
        "outputId": "aa816167-1234-4823-b062-6c8fc6fe9144"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 1. Generate linearly separable data with 3 non-separable points\n",
        "np.random.seed(42)\n",
        "X = np.r_[\n",
        "    np.random.randn(20, 2) - [2, 2],\n",
        "    np.random.randn(20, 2) + [2, 2]\n",
        "]\n",
        "y = np.array([0]*20 + [1]*20)\n",
        "\n",
        "\n",
        "# Mark 3 candidates non-separable points, DO NOT add them to the classifier yet\n",
        "X1 = np.vstack([[[0, 0], [1, -1], [-1, 1]]])\n",
        "y1 = np.hstack([[0, 1, 0]])  # Assign classes\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X1[:, 0], X1[:, 1], c=y1, cmap='bwr', edgecolors='k', s=230)\n",
        "\n",
        "####################### TWO LINES TO UNCOMMENT ########################\n",
        "############ THEY ADD THE 3 POINTS TO THE CLASSIFIER DATA #############\n",
        "\n",
        "#X = np.vstack([X, X1])\n",
        "#y = np.hstack([y, y1])  # Assign classes\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. Train a linear SVM classifier (clf)\n",
        "clf = SVC(kernel='linear', C=1e5)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Extract parameters w and b from the trained model\n",
        "w = clf.coef_[0]      # [w1, w2]\n",
        "b = clf.intercept_[0] # scalar bias\n",
        "\n",
        "# 3. Create a grid of x-values for plotting the decision boundary and margins\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "x_plot = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "# Decision boundary: w^T x + b = 0 ==> x2 = -(w1*x1 + b)/w2\n",
        "def line_y(x, offset=0.0):\n",
        "    return -(w[0]*x + (b - offset)) / w[1]\n",
        "\n",
        "y_decision = line_y(x_plot, offset=0)\n",
        "y_margin_plus = line_y(x_plot, offset=1)   # w^T x + b = 1\n",
        "y_margin_minus = line_y(x_plot, offset=-1) # w^T x + b = -1\n",
        "\n",
        "# 4. Plot the data points\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k', s=70)\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(X[:, 1].min()-1, X[:, 1].max()+1)\n",
        "plt.title('SVM with Slack Variables')\n",
        "\n",
        "# Plot the decision boundary (solid line)\n",
        "plt.plot(x_plot, y_decision, 'k-')\n",
        "# Plot the margins (dashed lines)\n",
        "plt.plot(x_plot, y_margin_plus, 'k--')\n",
        "plt.plot(x_plot, y_margin_minus, 'k--')\n",
        "\n",
        "# Label the decision boundary and margins\n",
        "x_text = x_max - 0.5\n",
        "y_text_decision = line_y(x_text, offset=0)\n",
        "plt.text(x_text, y_text_decision + 0.3, r'$w^T x + b = 0$', fontsize=12)\n",
        "\n",
        "y_text_plus = line_y(x_text, offset=1)\n",
        "plt.text(x_text, y_text_plus + 0.3, r'$w^T x + b = 1$', fontsize=12)\n",
        "\n",
        "y_text_minus = line_y(x_text, offset=-1)\n",
        "plt.text(x_text, y_text_minus + 0.3, r'$w^T x + b = -1$', fontsize=12)\n",
        "\n",
        "# 6. Draw and label the orthogonal distance 1/||w||\n",
        "x_center = (x_min + x_max) / 2\n",
        "y_center = line_y(x_center, offset=0)\n",
        "center_point = np.array([x_center, y_center])\n",
        "\n",
        "w_norm = np.linalg.norm(w)\n",
        "direction = w / w_norm\n",
        "\n",
        "margin_point = center_point + (1 / w_norm) * direction\n",
        "\n",
        "plt.plot([center_point[0], margin_point[0]],\n",
        "         [center_point[1], margin_point[1]], 'r-', lw=2)\n",
        "\n",
        "mid_point = (center_point + margin_point) / 2\n",
        "plt.text(mid_point[0] + 0.1, mid_point[1], r'$\\frac{1}{\\|w\\|}$', color='r', fontsize=12)\n",
        "\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r8Rg5dzrb4sA",
      "metadata": {
        "id": "r8Rg5dzrb4sA"
      },
      "source": [
        "# Kernel Formulation\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "Consider the SVM optimization problem for the **non-separable case**:\n",
        "\n",
        "$$ \\min_{w, b, \\xi} \\left( \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i \\right) $$\n",
        "\n",
        "subject to the constraints:\n",
        "\n",
        "$$ y_i (w^\\top x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i. $$\n",
        "\n",
        "If we attempt to map the input data $x$ into a higher-dimensional feature space using a transformation $x \\rightarrow \\phi(x)$, this would lead to a highly complex optimization problem. This complexity arises because the primal formulation directly involves both $\\|w\\|$ and the explicit mapping $\\phi(x)$, making the computations in high-dimensional spaces computationally expensive and often infeasible (especially for infinite-dimensional transforms).\n",
        "\n",
        "To efficiently handle such cases, the **dual formulation** of SVM is preferred, as it allows the use of kernel functions to compute inner products in the transformed space without explicitly calculating $\\phi(x)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A0K_0wB2ddQ_",
      "metadata": {
        "id": "A0K_0wB2ddQ_"
      },
      "source": [
        "## The Dual Problem\n",
        "\n",
        "The dual problem flips the optimization perspective. Instead of optimizing over $x$, it optimizes over the Lagrange multipliers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4W_7y58agkOr",
      "metadata": {
        "id": "4W_7y58agkOr"
      },
      "source": [
        "### The Dual Problem for Soft Margin SVM\n",
        "\n",
        "We can derive the **dual problem** to be:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j (x_i^\\top x_j)\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C\n",
        "$$\n",
        "\n",
        "This formulation depends **only on inner products** $x_i^\\top x_j$, making it compatible with the **kernel trick**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7864xzghHeE",
      "metadata": {
        "id": "a7864xzghHeE"
      },
      "source": [
        "\n",
        "### The Mapping $\\phi$\n",
        "\n",
        "Suppose we want to map $x$ into a higher-dimensional space, or even an infinite-dimensional space, using the mapping $x \\rightarrow \\phi(x)$. The linear decision boundaries in the high dimensional space would transform into complex and non-nlinear decision boundaries in our original data space.\n",
        "\n",
        "After applying the mapping $x \\rightarrow \\phi(x)$, the dual problem becomes:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j (\\phi(x_i)^T \\phi(x_j))\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C\n",
        "$$\n",
        "\n",
        "Here, the inner product $x_i^T x_j$ is replaced by $\\phi(x_i)^T \\phi(x_j)$, representing the inner product in the higher-dimensional feature space. However, using the **kernel trick**, we can compute this inner product directly through a kernel function $K(x_i, x_j)$ without explicitly calculating $\\phi(x)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9EMnQnzOhuvh",
      "metadata": {
        "id": "9EMnQnzOhuvh"
      },
      "source": [
        "### The Kernelized Dual Formula\n",
        "\n",
        "In the dual problem we will use the kernel function:\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j)\n",
        "$$\n",
        "\n",
        "This substitution allows the SVM to operate ***as if* the data were mapped into a higher-dimensional space**. However, the beauty of the **kernel trick** lies in the fact that **we do not need to compute this $x \\rightarrow \\phi(x)$ mapping explicitly**. This approach drastically reduces computational complexity and makes it feasible to work with high or even infinite-dimensional spaces.\n",
        "\n",
        "As a result, the kernelized dual problem becomes:\n",
        "\n",
        "$$\n",
        "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{n} \\alpha_i y_i = 0, \\quad 0 \\leq \\alpha_i \\leq C\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U-7CJtSLfLSl",
      "metadata": {
        "id": "U-7CJtSLfLSl"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### The RBF Kernel\n",
        "\n",
        "The **Radial Basis Function (RBF) kernel**, also known as the **Gaussian kernel**, is defined as:\n",
        "\n",
        "$$\n",
        "K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "\n",
        "In the context of SVMs, the RBF kernel enables the model to create **nonlinear decision boundaries** by implicitly mapping the input data into an **infinite-dimensional feature space**.\n",
        "\n",
        "This kernelized dual formulation allows SVMs to effectively model complex, nonlinear patterns in data while avoiding the computational burden of explicit feature mapping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CVog3C7sj930",
      "metadata": {
        "id": "CVog3C7sj930"
      },
      "source": [
        "### The $\\phi$ Behind the RBF Kernel\n",
        "\n",
        "The RBF kernel implicitly maps input vectors into an infinite-dimensional feature space. Although the explicit mapping $\\phi(x)$ is typically not computed due to its complexity, we can understand its structure by expanding the kernel using the Taylor series."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TGSBlplSilg7",
      "metadata": {
        "id": "TGSBlplSilg7"
      },
      "source": [
        "\n",
        "\n",
        "#### Explicit Mapping $\\phi(x)$ (Conceptual Form)\n",
        "\n",
        "For a one-dimensional input $x \\in \\mathbb{R}$, the feature mapping can be represented as:\n",
        "\n",
        "$$\n",
        "\\phi(x) = \\left[ e^{-\\frac{x^2}{2\\sigma^2}}, \\frac{x}{\\sigma}e^{-\\frac{x^2}{2\\sigma^2}}, \\frac{x^2}{\\sigma^2}e^{-\\frac{x^2}{2\\sigma^2}}, \\frac{x^3}{\\sigma^3}e^{-\\frac{x^2}{2\\sigma^2}}, \\dots \\right]\n",
        "$$\n",
        "\n",
        "For higher-dimensional inputs, the mapping includes all possible combinations of powers of input features.\n",
        "This means that the mapping not only considers individual powers of each feature but also includes **interaction terms between different features**.\n",
        "\n",
        "For example, if you have a 2D input $x = (x_1, x_2)$, the feature mapping would include terms like:\n",
        "\n",
        "$$\n",
        "e^{-\\frac{\\|x\\|^2}{2\\sigma^2}}, \\quad x_1 e^{-\\frac{\\|x\\|^2}{2\\sigma^2}}, \\quad x_2 e^{-\\frac{\\|x\\|^2}{2\\sigma^2}}, \\quad x_1^2 e^{-\\frac{\\|x\\|^2}{2\\sigma^2}}, \\quad x_1 x_2 e^{-\\frac{\\|x\\|^2}{2\\sigma^2}}, \\quad x_2^2 e^{-\\frac{\\|x\\|^2}{2\\sigma^2}}, \\dots\n",
        "$$\n",
        "\n",
        "The inclusion of cross terms like $x_1 x_2$ captures interactions between features, effectively **modeling dependencies**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LGtYhWNWkAUW",
      "metadata": {
        "id": "LGtYhWNWkAUW"
      },
      "source": [
        "\n",
        "#### Why This Matters?\n",
        "\n",
        "- **Complex Relationships:** The RBF kernel can model complex, nonlinear relationships between features without the need for manual feature engineering.\n",
        "\n",
        "- **Infinite Dimensions:** Since the Taylor series expansion of the exponential function includes terms of all degrees (up to infinity), the mapping accounts for interactions of any order.\n",
        "\n",
        "- **Kernel Trick:** Even though the mapping implicitly captures dependencies, the **kernel trick** allows these computations without explicitly calculating the mapping. The kernel function directly computes the inner product in the infinite-dimensional space, making the process efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hy15IQDjqVAE",
      "metadata": {
        "id": "Hy15IQDjqVAE"
      },
      "source": [
        "### Other Kernels\n",
        "\n",
        "1. **Polynomial Kernel**\n",
        "\n",
        "  The **polynomial kernel** enables the model to capture curved decision boundaries by considering feature interactions up to a specific degree.\n",
        "\n",
        "  $$\n",
        "  K(x, x') = (x^T x' + c)^d\n",
        "  $$\n",
        "\n",
        "  - **$d$** → degree of the polynomial  \n",
        "  - **$c$** → a constant (commonly set to 0 or 1)\n",
        "\n",
        "\n",
        "\n",
        "2. **Sigmoid Kernel (Hyperbolic Tangent Kernel)**\n",
        "\n",
        "  Inspired by neural networks, the **sigmoid kernel** uses the hyperbolic tangent function and can behave similarly to multi-layer perceptrons.\n",
        "\n",
        "  $$\n",
        "  K(x, x') = \\tanh(\\alpha x^T x' + c)\n",
        "  $$\n",
        "\n",
        "  - **$\\alpha$** → scaling parameter  \n",
        "  - **$c$** → shift parameter\n",
        "\n",
        "  *Note:* This kernel is **not positive semi-definite** for some parameter settings, so it should be used with caution.\n",
        "\n",
        "\n",
        "\n",
        "3. **Laplacian Kernel**\n",
        "\n",
        "  The **Laplacian kernel** resembles the RBF kernel but uses the **L1 norm** instead of the squared Euclidean distance, resulting in sharper decision boundaries.\n",
        "\n",
        "  $$\n",
        "  K(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|_1}{\\sigma}\\right)\n",
        "  $$\n",
        "\n",
        "  - **$\\|x - x'\\|_1$** → Manhattan (L1) distance  \n",
        "  - **$\\sigma$** → scale parameter\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZREskWE_uppB",
      "metadata": {
        "id": "ZREskWE_uppB"
      },
      "source": [
        "## Do We Need $w$ and $\\phi(x)$ for Classifying a New Point?\n",
        "\n",
        "\n",
        "In **Support Vector Machines (SVMs)**, the decision function for classifying a new point $x$ is:\n",
        "\n",
        "$$\n",
        "f(x) = \\text{sign}(w^T \\phi(x) + b)\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "- **$w$** is the weight vector in the feature space.  \n",
        "- **$\\phi(x)$** is the mapped version of $x$ in that feature space.\n",
        "\n",
        "At first glance, it seems like you would need to compute both $w$ and $\\phi(x)$ explicitly. **But the kernel trick avoids this.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fyXmiEYVuj1f",
      "metadata": {
        "id": "fyXmiEYVuj1f"
      },
      "source": [
        "\n",
        "\n",
        "### How the Kernel Trick Works in the Decision Function\n",
        "\n",
        "In the **dual formulation** of the SVM, the weight vector $w$ can be expressed as a linear combination of the training examples:\n",
        "\n",
        "$$\n",
        "w = \\sum_{i=1}^{n} \\alpha_i y_i \\phi(x_i)\n",
        "$$\n",
        "\n",
        "Substitute this back into the decision function:\n",
        "\n",
        "$$\n",
        "f(x) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i \\langle \\phi(x_i), \\phi(x) \\rangle + b\\right)\n",
        "$$\n",
        "\n",
        "**Here’s the trick:**\n",
        "\n",
        "The term $\\langle \\phi(x_i), \\phi(x) \\rangle$ is **exactly what the kernel function computes**:\n",
        "\n",
        "$$\n",
        "K(x_i, x) = \\langle \\phi(x_i), \\phi(x) \\rangle\n",
        "$$\n",
        "\n",
        "**This means:**\n",
        "\n",
        "- **No need to compute $w$ explicitly.**  \n",
        "- **No need to compute $\\phi(x)$ explicitly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XxdbdkHXu5Ls",
      "metadata": {
        "id": "XxdbdkHXu5Ls"
      },
      "source": [
        "\n",
        "### Final Decision Function Using Only Kernels\n",
        "\n",
        "The final decision function is:\n",
        "\n",
        "$$\n",
        "f(x) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i K(x_i, x) + b\\right)\n",
        "$$\n",
        "\n",
        "This function uses only:\n",
        "- **$n$**: number of training samples  \n",
        "- $K(x_i, x)$: **The kernel evaluations** between the new point $x$ and each training point $x_i$\n",
        "- **The Lagrange multipliers** $\\alpha_i$,  \n",
        "- **The class labels** $y_i$,  \n",
        "- And **the bias term** $b$.\n",
        "\n",
        "At first glance it looks like we traded the neeed to compute the mapping $\\phi$ explicitly for the prohibitive computational complexity: it seems we need to compute $K(x_i, x)$ for all training samples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s2nIkaF8vm0s",
      "metadata": {
        "id": "s2nIkaF8vm0s"
      },
      "source": [
        "### The Note on Support Vectors\n",
        "\n",
        "\n",
        "Note, that **$\\alpha_i$** is non-zero only for **support vectors** in the decision function.\n",
        "\n",
        "So how many support vectors can there be?\n",
        "\n",
        "- In a **linear SVM**, if data is linearly separable in $\\mathbb{R}^d$, you can separate the classes using at most **$d + 1$** support vectors, similar to how **$d + 1$** points define a hyperplane in $d$-dimensions.\n",
        "\n",
        "- However, when using **nonlinear kernels** (e.g., **polynomial** or **RBF**), the data is mapped into **higher** or even **infinite-dimensional** spaces. In these cases:\n",
        "\n",
        "- The **number of support vectors** can **increase** because more points may now lie close to the decision boundary in the transformed space.\n",
        "\n",
        "- The **RBF kernel**, for instance, can result in **many** or even **all data points** becoming support vectors, especially if the **regularization parameter $C$** is **high** or the data is **complex**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GgT59pTgodP1",
      "metadata": {
        "id": "GgT59pTgodP1"
      },
      "source": [
        "### Polynomial Kernel Example\n",
        "\n",
        "Recall the general form of a polynomial kernel is:\n",
        "\n",
        "$$\n",
        "K(x, x') = (x^T x' + c)^d\n",
        "$$\n",
        "\n",
        "For the **2nd-degree polynomial kernel** ($d = 2$) and assuming $c = 0$ for simplicity, the kernel becomes:\n",
        "\n",
        "$$\n",
        "K(x, x') = (x^T x')^2\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "#### Explicit Feature Mapping $\\phi(x)$\n",
        "\n",
        "For a simple 2D input vector $x = (x_1, x_2)$, let's expand the kernel:\n",
        "\n",
        "$$\n",
        "K(x, x') = (x_1 x_1' + x_2 x_2')^2 = x_1^2 x_1'^2 + 2x_1 x_1' x_2 x_2' + x_2^2 x_2'^2.\n",
        "$$\n",
        "\n",
        "\n",
        "This suggests that the explicit feature mapping $\\phi(x)$ is:\n",
        "\n",
        "$$\n",
        "\\phi(x) = [x_1^2, \\sqrt{2} x_1 x_2, x_2^2].\n",
        "$$\n",
        "\n",
        "\n",
        "#### Task\n",
        "Check it out yourself, by taking an inner product of\n",
        "\n",
        "$$\n",
        "\\phi(x) = [x_1^2, \\sqrt{2} x_1 x_2, x_2^2]\n",
        "$$\n",
        "and\n",
        "\n",
        "$$\n",
        "\\phi(x') = [{x'}_1^2, \\sqrt{2} {x'}_1 {x'}_2, {x'}_2^2].\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BTB0hvvM0sWs",
      "metadata": {
        "id": "BTB0hvvM0sWs"
      },
      "source": [
        "### Task\n",
        "\n",
        "How would you plot the decision boundary which is highly complex and non-linear, like with our 2nd-degree polynomial kernel?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Rkl0IR9GO1u",
      "metadata": {
        "id": "0Rkl0IR9GO1u"
      },
      "source": [
        "### Visualising SVM Decision boundary\n",
        "\n",
        "Below we will visualise the 2nd-degree polynomial kernel decision boundary for some artificially generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aiAvl_zTzmKS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "aiAvl_zTzmKS",
        "outputId": "7f737391-f973-40f9-ba8f-67fa32640b60"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# 1. Generate Data (Concentric Circles for Non-Linear Separation)\n",
        "np.random.seed(42)\n",
        "X, y = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "\n",
        "# Convert labels from (0,1) to (-1,1) for SVM\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# 2. Train SVM with Polynomial Kernel (Degree 2, c=0)\n",
        "clf = SVC(kernel='poly', degree=2, C=1.0, coef0=0, gamma=1)\n",
        "# scikit uses a complex scaled gamma for the kernels. Here we want to use non-optimal (but simplest!) kernels\n",
        "# K(x, x') = (x^T x')^2\n",
        "clf.fit(X, y)\n",
        "\n",
        "# 3. Create Meshgrid for Plotting Decision Boundary\n",
        "x_min, x_max = X[:, 0].min() - 0.2, X[:, 0].max() + 0.2\n",
        "y_min, y_max = X[:, 1].min() - 0.2, X[:, 1].max() + 0.2\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n",
        "\n",
        "# Flatten and Stack Grid for Prediction\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "decision = clf.decision_function(grid)\n",
        "decision = decision.reshape(xx.shape)\n",
        "\n",
        "# 4. Plot Data Points and Decision Boundary\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k', s=50, alpha=0.7)\n",
        "\n",
        "# Plot Decision Boundary and Margins\n",
        "plt.contour(xx, yy, decision, levels=[0], linewidths=2, colors='k')\n",
        "plt.contour(xx, yy, decision, levels=[-1, 1], linestyles='--', colors='grey')\n",
        "\n",
        "# Highlight Support Vectors\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=100, linewidth=1, facecolors='none', edgecolors='k', label='Support Vectors')\n",
        "\n",
        "plt.title('SVM with Degree 2 Polynomial Kernel')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.gca().set_aspect('equal')  #  making the plot square\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ky0TbIlw6na-",
      "metadata": {
        "id": "Ky0TbIlw6na-"
      },
      "source": [
        "### Task\n",
        "\n",
        "By the way, if you were not to use the kernels, but rather engineer features allowing for the linear separation of the points, what would it be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IRwGgq_H2uBa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "IRwGgq_H2uBa",
        "outputId": "1fb24567-a170-4f81-fc98-f54b09baee0b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3D Plot for phi(x) with Decision Boundary\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# 5. Visualize Explicit Mapping phi(x) = [x1^2, sqrt(2) x1 x2, x2^2]\n",
        "\n",
        "# Apply phi(x) to each point\n",
        "phi_X = np.array([[x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2] for x in X])\n",
        "\n",
        "# Apply phi(x) to the meshgrid for decision boundary\n",
        "phi_grid = np.array([[x[0]**2, np.sqrt(2)*x[0]*x[1], x[1]**2] for x in grid])\n",
        "\n",
        "# Scatter plot of transformed data\n",
        "ax.scatter(phi_X[:, 0], phi_X[:, 1], phi_X[:, 2], c=y, cmap='bwr', edgecolors='k', s=50, alpha=0.7, label='Data Points')\n",
        "\n",
        "# Plot only the decision boundary contour (i.e. where decision == 0)\n",
        "grid_decision = clf.decision_function(grid)\n",
        "contour_mask = np.isclose(grid_decision, 0, atol=0.01)  # select points near decision boundary\n",
        "ax.scatter(phi_grid[contour_mask, 0], phi_grid[contour_mask, 1], phi_grid[contour_mask, 2],\n",
        "           c='k', s=5, label='Decision Boundary')\n",
        "\n",
        "# Set axis labels and title for 3D plot\n",
        "ax.set_title('Explicit Mapping $\\phi(x) = [x_1^2, \\sqrt{2} x_1 x_2, x_2^2]$ with Decision Boundary')\n",
        "ax.set_xlabel('$x_1^2$')\n",
        "ax.set_ylabel('$\\sqrt{2} x_1 x_2$')\n",
        "ax.set_zlabel('$x_2^2$')\n",
        "\n",
        "# Add legend and grid\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vWQ6h3v8GmMZ",
      "metadata": {
        "id": "vWQ6h3v8GmMZ"
      },
      "source": [
        "# **Homework Assignment: Polynomial Kernel SVM in 3D Feature Space**\n",
        "-------------------------------\n",
        "You have seen how a polynomial kernel of degree 2 can map the original 2D data\n",
        "$$(x_1, x_2)$$\n",
        "to a 3D feature space via some transformation\n",
        "$$\n",
        "\\phi(\\mathbf{x}).\n",
        "$$\n",
        "You also saw that **the decision boundary** in the 3D space—expected to be a hyperplane—**appears** as a circular surface when visualized.\n",
        "\n",
        "## **Primary Question**\n",
        "\n",
        "**Why does the SVM's decision boundary in the higher-dimensional (3D) feature space *not* look like a “flat” hyperplane,**\n",
        "even though we *know* the classifier’s separating surface *is* mathematically linear in that space?\n",
        "\n",
        "## **Secondary Question**\n",
        "\n",
        "Explain **why** a hyperplane in 3D becomes a **circular (or elliptical) curve** when viewed back in the original 2D plane of the dataset.\n",
        "\n",
        "## **Task & Deliverables**\n",
        "\n",
        "1. **Colab Notebook**  \n",
        "   - Create a **Colab notebook** that:\n",
        "     - Clearly **answers** both the **primary** and **secondary** questions with appropriate plots and short explanations.\n",
        "     - Uses Python code to visualise and create plots.\n",
        "   - Include a discussion (in markdown cells) illustrating the geometry behind the mapping\n",
        "     $$\\phi(\\mathbf{x})$$\n",
        "     and the resulting decision boundary.\n",
        "   - Use\n",
        "     `$` symbol to delimit inline equations and `$$` to delimit full-line equations.\n",
        "\n",
        "2. **Publish on GitHub**  \n",
        "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WvGtT5OsBU34",
      "metadata": {
        "id": "WvGtT5OsBU34"
      },
      "source": [
        "# Kernel Ridge Regression and Kernel Logistic Regression\n",
        "----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b-8k4ib5BbKI",
      "metadata": {
        "id": "b-8k4ib5BbKI"
      },
      "source": [
        "## SVM Primal Loss: Hinge Loss + Penalty Reformulation\n",
        "\n",
        "Recall the **primal form** of the **Soft-Margin SVM** optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
        "$$\n",
        "\n",
        "subject to:\n",
        "\n",
        "$$\n",
        "y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- **$ \\xi_i $**: Slack variables allowing for misclassifications.\n",
        "- **$ C $**: Regularization parameter balancing margin size and misclassification penalty.\n",
        "\n",
        "We embed the constraints into the objective using the **positive part operator** $ [x]_+ = \\max(0, x)$ and express $C$ as the inverse of the regularization parameter $\\lambda$, $C = \\frac{1}{\\lambda}$.\n",
        "\n",
        "\n",
        "Substituting this into the primal loss gives the equivalent optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{w, b} \\frac{\\lambda}{2} \\|w\\|^2 + \\sum_{i=1}^{n} [1 - y_i (w^T x_i + b)]_+\n",
        "$$\n",
        "\n",
        "### Interpreting the Terms\n",
        "\n",
        "1. **Penalty Term (Regularization):**\n",
        "\n",
        "   $$\n",
        "   \\frac{\\lambda}{2} \\|w\\|^2\n",
        "   $$\n",
        "\n",
        "   This term penalizes large weights, effectively controlling the **margin width**. A smaller $\\lambda$ leads to less regularization (allowing more flexibility), while a larger $\\lambda$ forces the model to maximize the margin more aggressively.\n",
        "\n",
        "2. **Hinge Loss Term:**\n",
        "\n",
        "   $$\n",
        "   \\sum_{i=1}^{n} [1 - y_i f(x_i)]_+\n",
        "   $$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HXuCwWLTBw_s",
      "metadata": {
        "id": "HXuCwWLTBw_s"
      },
      "source": [
        "### Understanding the Hinge Loss\n",
        "\n",
        "- If a data point is **correctly classified** and lies **outside the margin** ($ y_i f(x_i) \\geq 1 $), the hinge loss is zero:\n",
        "\n",
        "$$\n",
        "[1 - y_i f(x_i)]_+ = 0\n",
        "$$\n",
        "\n",
        "- If a data point lies **inside the margin** ($ y_i f(x_i) < 1 $) or is **misclassified** ($ y_i f(x_i) < 0 $), the loss is proportional to its distance from the margin.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "om2n1caGCgwc",
      "metadata": {
        "id": "om2n1caGCgwc"
      },
      "source": [
        "\n",
        "\n",
        "## Substituting Hinge Loss with Other Loss Functions\n",
        "\n",
        "While SVM uses the **hinge loss** for classification, other loss functions can be substituted, leading to different model behaviors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. **Squared Error Loss**\n",
        "\n",
        "  Common in **regression tasks** and **Kernel Ridge Regression**:\n",
        "\n",
        "  $$\n",
        "  L_{\\text{square}}(y_i, f(x_i)) = (y_i - f(x_i))^2\n",
        "  $$\n",
        "\n",
        "  When the **hinge loss** is replaced with **squared error loss** and combined with the **kernel trick**, the model transitions into **Kernel Ridge Regression**.\n",
        "\n",
        "  This changes the focus from **classification** to **function approximation**, allowing for more flexible decision boundaries adapted to regression tasks.\n",
        "\n",
        "2. **Binomial Log Loss (Logistic Loss)**\n",
        "\n",
        "  Used in **logistic regression**, this loss is smooth and differentiable:\n",
        "\n",
        "  $$\n",
        "  L_{\\text{log}}(y_i, f(x_i)) = \\log(1 + \\exp(-y_i f(x_i)))\n",
        "  $$\n",
        "\n",
        "  While replacing the hinge loss with squared error loss leads to Kernel Ridge Regression (suitable for regression tasks), replacing the hinge loss with the **logistic loss** results in a probabilistic classifier known as **Kernel Logistic Regression**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AVpNjnQ-Fu8K",
      "metadata": {
        "id": "AVpNjnQ-Fu8K"
      },
      "source": [
        "## Comparing Linear Ridge Regression and Kernel Ridge Regression\n",
        "\n",
        "In this analysis, we aim to compare the performance of **Linear Ridge Regression** and **Kernel Ridge Regression (KRR)**, with a focus on the impact of using **Radial Basis Function (RBF) kernels**. While **Linear Ridge Regression** fits a simple linear relationship between features and the target variable, **Kernel Ridge Regression** leverages the **kernel trick** to model complex, non-linear patterns without explicitly transforming the feature space.\n",
        "\n",
        "For this comparison, we will use a **synthetic sinusoidal dataset**, designed to highlight the differences between linear and non-linear modeling approaches. The dataset consists of data points sampled from a **sine wave** with added **Gaussian noise**. This creates a non-linear regression problem that linear models struggle to capture accurately.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xyDX33jwEMD6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "xyDX33jwEMD6",
        "outputId": "d34f108c-069e-42c4-b9e6-ac5aac19e2fb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Generate Non-Linear Data\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(100, 1), axis=0)  # Random X values between 0 and 5\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])  # Non-linear sine wave with noise\n",
        "\n",
        "# Create dense grid for plotting predictions\n",
        "X_plot = np.linspace(0, 5, 500).reshape(-1, 1)\n",
        "\n",
        "# 2. Train Linear Ridge Regression (Baseline)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X, y)\n",
        "y_ridge = ridge.predict(X_plot)\n",
        "\n",
        "# 3. Train Kernel Ridge Regression with RBF Kernel\n",
        "krr = KernelRidge(kernel='rbf', alpha=1.0)  # Gamma non-specified, i.e. data-adjusted. Higher gamma captures finer details\n",
        "krr.fit(X, y)\n",
        "y_krr = krr.predict(X_plot)\n",
        "\n",
        "# 4. Plot Results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Original data\n",
        "plt.scatter(X, y, color='gray', label='Training data', edgecolor='k', alpha=0.6)\n",
        "\n",
        "# Linear Ridge Regression\n",
        "plt.plot(X_plot, y_ridge, color='blue', linewidth=2, label='Linear Ridge Regression')\n",
        "\n",
        "# Kernel Ridge Regression\n",
        "plt.plot(X_plot, y_krr, color='red', linewidth=2, label='Kernel Ridge Regression (RBF)')\n",
        "\n",
        "# Ground truth sine function (for reference)\n",
        "plt.plot(X_plot, np.sin(X_plot), color='green', linestyle='--', label='True function (sin(x))')\n",
        "\n",
        "# Plot formatting\n",
        "plt.title('Kernel Ridge Regression vs. Linear Ridge Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 5. Evaluate and Compare\n",
        "ridge_mse = mean_squared_error(y, ridge.predict(X))\n",
        "krr_mse = mean_squared_error(y, krr.predict(X))\n",
        "print(f\"Linear Ridge Regression MSE: {ridge_mse:.3f}\")\n",
        "print(f\"Kernel Ridge Regression MSE: {krr_mse:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
