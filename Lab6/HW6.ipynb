{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework Assignment: Understanding Splitting Criteria in CART for Regression**\n",
    "---------------------\n",
    "\n",
    "Homework6: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Kos261/ML25/blob/main/Lab6/HW6.ipynb)\n",
    "\n",
    "\n",
    "In this assignment, you will explore three common formulations of the splitting criterion used in **CART (Classification and Regression Trees)** for **regression problems**:\n",
    "\n",
    "1. **Local RSS Minimization**  \n",
    "2. **RSS Gain Maximization**  \n",
    "3. **Total RSS Minimization**\n",
    "\n",
    "You will investigate whether any of these criteria are equivalent, and you will design an experiment to determine which criterion is actually employed in a standard implementation such as **scikit-learn’s DecisionTreeRegressor**.\n",
    "\n",
    "\n",
    "\n",
    "## **The Problem**\n",
    "\n",
    "Many treatments of CART for regression describe the split selection process in different ways. Below are three frequently cited formulations. Suppose we have a dataset with features $X$ and target $y$, and we seek to choose a feature $X_j$ and a threshold $t$ to split the data into two regions $R_1(X_j, t)$ and $R_2(X_j, t)$. Denote by $\\bar{y}_{R_m}$ the mean of targets within region $R_m$.\n",
    "\n",
    "1. **Local RSS Minimization**  \n",
    "   We select the feature and threshold that minimize the **sum of squared errors** in the two resulting child nodes:\n",
    "   $$\n",
    "   (X_j^*, t^*) = \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2.\n",
    "   $$\n",
    "\n",
    "2. **RSS Gain Maximization**  \n",
    "\n",
    "   It is also a local method, looking only at a parent and two child nodes.\n",
    "\n",
    "   We select the feature and threshold that maximize the **reduction** in RSS, computed by subtracting the RSS of the two child nodes from the RSS in the parent node:\n",
    "   $$\n",
    "   (X_j^*, t^*) = \\arg\\max_{X_j, t} \\Bigl\\{\n",
    "   \\underbrace{\\sum_{i : x_i \\in \\text{Parent}} (y_i - \\bar{y})^2}_{\\text{Parent RSS}}\n",
    "   \\;-\\;\n",
    "   \\underbrace{\\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2}_{\\text{Children RSS}}\n",
    "   \\Bigr\\}.\n",
    "   $$\n",
    "\n",
    "3. **Total RSS Minimization**  \n",
    "   For a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with features $X$ and target $y$, let $T$ be the current tree.\n",
    "\n",
    "   For any split on feature $X_j$ at threshold $t$, define $T(X_j, t)$ as the new tree obtained by splitting one leaf of $T$ into two leaves $R_1(X_j, t)$ and $R_2(X_j, t)$.\n",
    "   \n",
    "   Let $\\mathrm{Leaves}(T(X_j, t))$ be the set of all leaf indices in this new tree. For each leaf $m \\in \\mathrm{Leaves}(T(X_j, t))$, define:\n",
    "   $$\n",
    "   R_m = \\{\\, i \\,\\mid\\, x_i \\text{ ends in leaf } m\\}.\n",
    "   $$\n",
    "\n",
    "   $R_m$ set collects all data indices $i$ whose feature vector $x_i$ is classified into the leaf node $m$ when passed through the tree $T(X_j,t)$. In other words, each leaf node $m$ in $T(X_j, t)$ corresponds to a unique path of splits, and any data point $x_i$ that follows that path is assigned to the leaf $m$; hence, it belongs to $R_m$.\n",
    "\n",
    "   $R_m$ sets for all leafs $m \\in \\mathrm{Leaves}(T(X_j, t))$ define a partition of all indices.\n",
    "\n",
    "   Then the objective of **minimizing total Residual Sum of Squares (total RSS)** is stated as:\n",
    "   $$\n",
    "   (X_j^*, t^*) = \\arg\\min_{(X_j, t)} \\sum_{m \\in \\mathrm{Leaves}(T(X_j, t))}\n",
    "   \\sum_{i \\in R_m} \\Bigl(y_i - \\overline{y}_{R_m}\\Bigr)^2,\n",
    "   $$\n",
    "   where\n",
    "   $$\n",
    "   \\overline{y}_{R_m} = \\frac{1}{\\lvert R_m \\rvert}\n",
    "   \\sum_{i \\in R_m} y_i\n",
    "   $$\n",
    "   is the mean response in leaf $m$.\n",
    "\n",
    "\n",
    "## **Research Questions**\n",
    "\n",
    "1. **Equivalence Analysis**  \n",
    "   Determine whether the above formulations are equivalent or if they can yield different split choices. Specifically:\n",
    "   - Are *local RSS minimization* and *RSS gain maximization* equivalent?\n",
    "   - Does *total RSS minimization* coincide with either of these two, or is it distinct?\n",
    "   \n",
    "2. **Empirical Experiment**  \n",
    "   Design and conduct a Python experiment to determine which of these formulations is implemented in `scikit-learn` in `DecisionTreeRegressor`. Present numerical results and plots to support your conclusion.\n",
    "\n",
    "\n",
    "## **Tasks & Deliverables**\n",
    "\n",
    "1. **Formulation Analysis**  \n",
    "   - Compare *local RSS minimization*, *RSS gain maximization*, and *total RSS minimization*.\n",
    "   - If you find that any pair of formulations is equivalent, provide a concise proof.  \n",
    "   - If you find that they differ, construct a counterexample.\n",
    "\n",
    "2. **Empirical Verification**  \n",
    "   - Create a small artificial dataset and train a `DecisionTreeRegressor` from `scikit-learn`.\n",
    "   - The dataset must be designed in a way that uniquely identifies the formulation used. Provide a short code snippet and a plot or table to support your conclusion.\n",
    "\n",
    "3. **Report**  \n",
    "   - Summarize your theoretical insights and empirical findings in a **Colab notebook**.\n",
    "   - Include the relevant proofs, code, discussion, and conclusions.\n",
    "   - Place the notebook in your **GitHub repository** for this course, add a link to it in your README.md and add an **“Open in Colab”** badge in the notebook so it can be launched directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "We split parent into two regions $R_1$ and $R_2$\n",
    "\n",
    "$RSS_{parent} = \\sum_{y_i \\in parent} (y_i - \\bar{y})^2$\n",
    "\n",
    "$RSS_{children} = \\sum_{y_i \\in R_{1}} (y_i - \\bar{y}_{R_{1}})^2 + \\sum_{y_i \\in R_{2}} (y_i - \\bar{y}_{R_2})^2 $\n",
    "\n",
    "$RSS_{GAIN} = RSS_{parent} - RSS_{children}$    \n",
    "\n",
    "\n",
    "**Total RSS**\n",
    "Ex. Two leaves $A$ and $B$. Split $B$ into  $B_1$ and $B_2$\n",
    "\n",
    "1) $RSS_{total} = RSS(A) + RSS(B)$\n",
    "2) $RSS_{total} = RSS(A) + RSS(B_1) + RSS(B_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "Maxxing $RSS_{GAIN}$ when $RSS_{parent}$ is fixed (because it doesn't depend on treshold and chosen feature) means minimazing $RSS_{children}$ which means *local RSS minimization* and *RSS gain maximization* are equivalent.\n",
    "\n",
    "\n",
    "Total RSS is similar to other two when we calculate only current split and not any other splits. First two methods are **greedy** and do not track history. \n",
    "\n",
    "# ANSWER 2\n",
    "\n",
    "Checking sklearn's method:\n",
    "As a counterexample I created this syntethic dataset to be as weird as I could imagine in terms of amplifying differences between **greedy algorithm and backtracking methods** yet simple enough to track changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMPIRICAL EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_rss(y_subset):\n",
    "    if len(y_subset) == 0:\n",
    "        return 0\n",
    "    return np.sum((y_subset - np.mean(y_subset)) ** 2)\n",
    "\n",
    "def find_best_second_split(y_values):\n",
    "    if len(y_values) <= 1: \n",
    "        return compute_rss(y_values)\n",
    "    \n",
    "    best_rss = float('inf')\n",
    "    for i in range(1, len(y_values)):\n",
    "        left_rss = compute_rss(y_values[:i])\n",
    "        right_rss = compute_rss(y_values[i:])\n",
    "        total = left_rss + right_rss\n",
    "        if total < best_rss:\n",
    "            best_rss = total\n",
    "    \n",
    "    return best_rss\n",
    "\n",
    "# Dataset where Total RSS and Local RSS would choose different splits\n",
    "X = np.array([1, 2, 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10, 11, 12]).reshape(-1, 1)\n",
    "y = np.array([1, 1, 10, 10, 1 , 1 , 20, 20, 20, 1 , 20, 20])\n",
    "#    splits        ^       ^      ^            ^  ^\n",
    "splits =        [ 2.5,    4.5  , 6.5    ,    9.5, 10.5    ]\n",
    "\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree.fit(X, y)\n",
    "sklearn_first_split = tree.tree_.threshold[0]\n",
    "sklearn_second_split = tree.tree_.threshold[1]\n",
    "print(\"Sklearn's first split:\", sklearn_first_split)\n",
    "print(\"Sklearn's second split:\", sklearn_second_split)\n",
    "\n",
    "# Evaluate for local RSS (first split only)\n",
    "best_local_split = None\n",
    "best_local_rss = float('inf')\n",
    "\n",
    "for split in splits:\n",
    "    left_indices = [i for i in range(len(X)) if X[i] <= split]\n",
    "    right_indices = [i for i in range(len(X)) if X[i] > split]\n",
    "    \n",
    "    left_y = [y[i] for i in left_indices]\n",
    "    right_y = [y[i] for i in right_indices]\n",
    "        \n",
    "    left_rss = compute_rss(left_y)\n",
    "    right_rss = compute_rss(right_y)\n",
    "    \n",
    "    total_first_level_rss = left_rss + right_rss\n",
    "    \n",
    "    if total_first_level_rss < best_local_rss:\n",
    "        best_local_rss = total_first_level_rss\n",
    "        best_local_split = split\n",
    "\n",
    "print(\"Best Local RSS split (depth 1):\", best_local_split)\n",
    "print(\"Local RSS after first split:\", best_local_rss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ##### DEPTH 2 #####\n",
    "best_global_split = None\n",
    "best_global_rss = float('inf')\n",
    "\n",
    "\n",
    "for split in splits:\n",
    "\n",
    "    left_indices = [i for i in range(len(X)) if X[i] <= split]\n",
    "    right_indices = [i for i in range(len(X)) if X[i] > split]\n",
    "    \n",
    "    left_y = [y[i] for i in left_indices]\n",
    "    right_y = [y[i] for i in right_indices]\n",
    "\n",
    "    left_second_level_rss = find_best_second_split(left_y)\n",
    "    right_second_level_rss = find_best_second_split(right_y)\n",
    "    total_depth2_rss = left_second_level_rss + right_second_level_rss\n",
    "    \n",
    "    if total_depth2_rss < best_global_rss:\n",
    "        best_global_rss = total_depth2_rss\n",
    "        best_global_split = split\n",
    "\n",
    "print(\"Best Global RSS split (considering depth 2):\", best_global_split)\n",
    "print(\"Total RSS after depth 2 tree:\", best_global_rss)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y)\n",
    "plt.axvline(x=best_local_split, color='r', linestyle='--', label=f'Local Best Split (x={best_local_split})')\n",
    "plt.axvline(x=best_global_split, color='g', linestyle='--', label=f'Global Best Split (x={best_global_split})')\n",
    "plt.title('Dataset with Different Optimal Splits Based on Approach')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER\n",
    "\n",
    "Results might suggest that sclearn is using local RSS maximization. I think it's weird but who am I to decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.criterion)  # Output: 'squared_error' (which uses Local RSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
