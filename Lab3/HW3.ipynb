{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework Assignment: Optimizing the Classification Threshold in Logistic Regression**\n",
    "-------------------------------\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Kos261/ML25/blob/main/Lab3/HW3.ipynb)\n",
    "\n",
    "In this assignment, you will explore the concept of **classification thresholds** in **Logistic Regression** and investigate whether the commonly used threshold of **0.5** is always the optimal choice. To complete this assignment, you will need to conduct **additional reading** on **ROC curves** and the **AUC metric**, beyond the materials covered in class.\n",
    "\n",
    "This exercise focuses on the **importance of threshold selection** and how it impacts model performance based on different evaluation criteria.\n",
    "\n",
    "## **The Objective**\n",
    "\n",
    "**Is a 0.5 threshold always the best choice in Logistic Regression?**\n",
    "\n",
    "- Analyze if the standard threshold of **0.5** always yields the most desirable results in various scenarios.\n",
    "- Investigate alternative thresholds and how they can improve model performance depending on the problem context.\n",
    "\n",
    "## **Helper Questions**\n",
    "\n",
    "1. **Would a different threshold yield better results?**  \n",
    "   - How does shifting the threshold affect the **trade-off between sensitivity and specificity**?\n",
    "\n",
    "2. **How do you define \"better\" results?**  \n",
    "   - Is **higher specificity** more valuable than **higher sensitivity** in certain contexts? Or maybe the other way around?\n",
    "   - Does the **best threshold** depend on the task? (discuss cases like *disease detection* vs. *spam filtering*)\n",
    "\n",
    "3. **How does a ROC curve help in this process?**  \n",
    "   - Learn how to use a **ROC curve** to visualize the trade-off between **True Positive Rate (Sensitivity)** and **False Positive Rate (1 - Specificity)**.\n",
    "\n",
    "4. **How do you select the optimal threshold using the ROC curve?**  \n",
    "   - Discuss strategies to select a threshold depending on the specific task at hand and the acceptable **trade-off between sensitivity and specificity**.\n",
    "\n",
    "5. **What is the AUC metric and how is it useful?**  \n",
    "   - Define **AUC (Area Under the Curve)** and discuss its role in evaluating the overall performance of a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLES TPR and FPR for SPAM mail model\n",
    "\n",
    "**Definitions**\n",
    "\n",
    "TruePositiveRatio  (Sensitivity) = TP / TP + FN      \n",
    "FalsePositiveRatio (Specificity) = FP / FP + TN     \n",
    "\n",
    "TP - Is True and predicted True                   \n",
    "FN - should be True but predicted False         \n",
    "FP - should be False but predicted True             \n",
    "TN - Is false and predicted false           \n",
    "\n",
    "Treshold = 0.5\n",
    "\n",
    "| Actual \\ Predicted | Spam     | Not       |\n",
    "|--------------------|----------|-----------|\n",
    "| Spam               | 800 (TP) | 100 (FN)  |\n",
    "| Not                | 500 (FP) | 8600 (TN) |\n",
    "\n",
    "Treshold = 0.95\n",
    "\n",
    "| Actual \\ Predicted | Spam     | Not        |\n",
    "|--------------------|----------|------------|\n",
    "| Spam               | 200 (TP) | 700 (FN)   |\n",
    "| Not                | 10 (FP)  | 9090 (TN)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "When the treshold is higher the model assigns True label when its \"more confident\". But it detects fewer examples of the target class overall and has lower sensetivity.\n",
    "\n",
    "When the treshold is lower the model is less strict, it assigns True more often but makes lower precission and sometimes False Positive predicitons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 2\n",
    "The specificity and sensitivity ratio depends on problem. For example if we want to detect cancer cells we want to \"Dmuchać na zimne\" and preferably detect more. So sensitivity in this case is more preferable.\n",
    "\n",
    "However with detecting spam in emails we prefer specificity. Classifying good email as spam can really hurt. We often don't want to miss important information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "\n",
    "As shown in exaples, the ROC is the plot TPR against FPR with fixed treshold. Diagonal line means that our model is totally random, and \"Step\" plot means it's perfect model that never misses and there are no false alarms :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 4/5\n",
    "When model is completly random, the ROC AUC score (area under ROC curve) is ≈ 0.5 but when model is near perfection and it never misses prediction it's score is ≈ 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
