{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework Assignment: Analyzing and Plotting Bias in Penalized Regression**\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "In this assignment, you will explore how **Ridge** and **Lasso** regression introduce **bias** into a model to reduce **variance**, and how the choice of the regularization parameter $\\lambda$ affects this trade-off. The goal is to visualize and analyze the **bias-variance trade-off** and understand the conditions under which penalization helps or hinders model performance.\n",
    "\n",
    "## **The Question**\n",
    "\n",
    "**How does varying the regularization parameter $\\lambda$ in Ridge and Lasso regression impact the trade-off between bias and variance?**\n",
    "\n",
    "- Generate a synthetic dataset based on a **known** linear relationship:\n",
    "  \n",
    "  $$\n",
    "  y = \\beta_0 + \\beta_1 x + \\ldots + \\epsilon\n",
    "  $$\n",
    "\n",
    "  where $$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2),$$\n",
    "\n",
    "  use a **high-dimensional** setting (e.g., 50 predictors) with only a few non-zero true coefficients to emphasize the effects of regularization. I stress, the $\\beta_i$ coefficients should be known for this experiment and they should be mostly 0, with only a few non-zero parameters.\n",
    "\n",
    "- Investigate how increasing $\\lambda$ influences the model’s **bias**, **variance**, and **Mean Squared Error (MSE)**.\n",
    "- Plot **Bias²**, **Variance**, and **MSE** on a single graph for both Ridge and Lasso models.\n",
    "- Explain MSE decomposition into bias and variance. Read more on the MSE decomposition if you need to.\n",
    "\n",
    "**Does the regularization lead to an optimal trade-off point where MSE is minimized? Explain why this point exists.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Expected Outcome:**\n",
    " - As $\\lambda$ increases:\n",
    "   - **Bias** increases (the model becomes too simple).\n",
    "   - **Variance** decreases (the model becomes more stable).\n",
    "   - **MSE** forms a **U-shape**, revealing the optimal trade-off.\n",
    "\n",
    "- Analyze how **Ridge** and **Lasso** differ in terms of their bias-variance trade-offs.\n",
    "- Discuss situations where one method may outperform the other, considering factors like **feature sparsity** and **multicollinearity**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder \n",
    "\n",
    "$$\\text{Bias}^2(\\hat{\\theta}) = \\left(\\mathbb{E}[\\hat{\\theta}] - \\theta\\right)^2$$\n",
    "\n",
    "$$\\text{Var}(\\hat{\\theta}) = \\mathbb{E}\\left[ \\left(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] \\right)^2 \\right]$$\n",
    "\n",
    "$$\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}\\left[(\\hat{\\theta} - \\theta)^2\\right] \n",
    "= \\text{Var}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(101)\n",
    "\n",
    "beta_len = 50\n",
    "n_samples = 100\n",
    "\n",
    "beta = np.zeros(beta_len)\n",
    "beta[[0, 5, 11, 12, 33, 38, 49]] = [1.5, -2.5, -3.0, 1.0, 2.5, 0.8, 1.2]\n",
    "beta_0 = 5.0\n",
    "\n",
    "X = np.random.randn(n_samples, beta_len)\n",
    "Y = beta_0 + X.dot(beta) + np.random.normal(0, 1, n_samples)  # Adding noise\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, Y_train)\n",
    "Y_pred = ridge_model.predict(X_test)\n",
    "print(\"Predicted coefficients:\\n\", ridge_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 100\n",
    "alphas = np.logspace(-2, 4, 20)\n",
    "for_plot = []\n",
    "\n",
    "for a in alphas:\n",
    "    all_preds_Ridge = []\n",
    "    all_mses = []\n",
    "    \n",
    "    for run in range(runs):\n",
    "        new_epsilon = np.random.normal(0, 1, n_samples)\n",
    "        Y_new = beta_0 + X.dot(beta) + new_epsilon   \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y_new, test_size=0.3, random_state=run)\n",
    "        true_Y_test = beta_0 + X_test.dot(beta)\n",
    "        model = Ridge(alpha=a)\n",
    "        model.fit(X_train, Y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        all_preds_Ridge.append(preds)\n",
    "        \n",
    "        \n",
    "        all_mses.append(np.mean((preds - true_Y_test)**2))\n",
    "\n",
    "    all_preds_Ridge = np.array(all_preds_Ridge)\n",
    "    # true_Y_test = beta_0 + X_test.dot(beta)\n",
    "\n",
    "\n",
    "    mean_prediction = np.mean(all_preds_Ridge, axis=0)\n",
    "    bias_2 = np.mean((mean_prediction - true_Y_test)**2)\n",
    "    var = np.mean(np.var(all_preds_Ridge, axis=0))\n",
    "    mse = np.mean(all_mses)\n",
    "    expected_mse = bias_2 + var\n",
    "    \n",
    "    labda = n_samples * a\n",
    "    for_plot.append((labda, mean_prediction, bias_2, var, mse, expected_mse))\n",
    "    \n",
    "    print(f\"alpha: {a}\")\n",
    "    print(f\"Bias^2: {bias_2:.4f}\")\n",
    "    print(f\"Variance: {var:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"Expected MSE (Bias^2 + Variance): {expected_mse:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labdas = [item[0] for item in for_plot]\n",
    "bias_squared = [item[2] for item in for_plot]\n",
    "variances = [item[3] for item in for_plot]\n",
    "mses = [item[4] for item in for_plot]\n",
    "expected_mses = [item[5] for item in for_plot]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(labdas, bias_squared, label='Bias²', marker='o')\n",
    "plt.plot(labdas, variances, label='Variance', marker='o')\n",
    "plt.plot(labdas, mses, label='MSE', marker='o')\n",
    "plt.plot(labdas, expected_mses, label='Bias² + Variance', marker='x', linestyle='--')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda (Regularization Parameter)', fontsize=12)\n",
    "plt.ylabel('Error', fontsize=12)\n",
    "plt.title('Bias-Variance Trade-off for Ridge Regression', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER:\n",
    "Bigger labda increases MSE and lowers Variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
