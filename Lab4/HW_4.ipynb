{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework Assignment: Analyzing and Plotting Bias in Penalized Regression**\n",
    "Homework4: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Kos261/ML25/blob/main/Lab4/HW4.ipynb)\n",
    "-------------------------------\n",
    "\n",
    "In this assignment, you will explore how **Ridge** and **Lasso** regression introduce **bias** into a model to reduce **variance**, and how the choice of the regularization parameter $\\lambda$ affects this trade-off. The goal is to visualize and analyze the **bias-variance trade-off** and understand the conditions under which penalization helps or hinders model performance.\n",
    "\n",
    "## **The Question**\n",
    "\n",
    "**How does varying the regularization parameter $\\lambda$ in Ridge and Lasso regression impact the trade-off between bias and variance?**\n",
    "\n",
    "- Generate a synthetic dataset based on a **known** linear relationship:\n",
    "  \n",
    "  $$\n",
    "  y = \\beta_0 + \\beta_1 x + \\ldots + \\epsilon\n",
    "  $$\n",
    "\n",
    "  where $$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2),$$\n",
    "\n",
    "  use a **high-dimensional** setting (e.g., 50 predictors) with only a few non-zero true coefficients to emphasize the effects of regularization. I stress, the $\\beta_i$ coefficients should be known for this experiment and they should be mostly 0, with only a few non-zero parameters.\n",
    "\n",
    "- Investigate how increasing $\\lambda$ influences the model’s **bias**, **variance**, and **Mean Squared Error (MSE)**.\n",
    "- Plot **Bias²**, **Variance**, and **MSE** on a single graph for both Ridge and Lasso models.\n",
    "- Explain MSE decomposition into bias and variance. Read more on the MSE decomposition if you need to.\n",
    "\n",
    "**Does the regularization lead to an optimal trade-off point where MSE is minimized? Explain why this point exists.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Expected Outcome:**\n",
    " - As $\\lambda$ increases:\n",
    "   - **Bias** increases (the model becomes too simple).\n",
    "   - **Variance** decreases (the model becomes more stable).\n",
    "   - **MSE** forms a **U-shape**, revealing the optimal trade-off.\n",
    "\n",
    "- Analyze how **Ridge** and **Lasso** differ in terms of their bias-variance trade-offs.\n",
    "- Discuss situations where one method may outperform the other, considering factors like **feature sparsity** and **multicollinearity**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder \n",
    "\n",
    "$$\\text{Bias}^2(\\hat{\\theta}) = \\left(\\mathbb{E}[\\hat{\\theta}] - \\theta\\right)^2$$\n",
    "\n",
    "$$\\text{Var}(\\hat{\\theta}) = \\mathbb{E}\\left[ \\left(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}] \\right)^2 \\right]$$\n",
    "\n",
    "$$\\text{MSE}(\\hat{\\theta}) = \\mathbb{E}\\left[(\\hat{\\theta} - \\theta)^2\\right] \n",
    "= \\text{Var}(\\hat{\\theta}) + \\text{Bias}^2(\\hat{\\theta})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(101)\n",
    "\n",
    "beta_len = 50\n",
    "n_samples = 100\n",
    "\n",
    "beta = np.zeros(beta_len)\n",
    "beta[[0, 5, 11, 12, 33, 38, 49]] = [1.5, -2.5, -3.0, 1.0, 2.5, 0.8, 1.2]\n",
    "beta_0 = 5.0\n",
    "\n",
    "X = np.random.randn(n_samples, beta_len)\n",
    "Y = beta_0 + X.dot(beta) + np.random.normal(0, 1, n_samples)  # Adding noise\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bias_variance(model, X, y, alphas, n_repeats=100):\n",
    "    predictions = np.zeros((len(alphas), len(y), n_repeats))\n",
    "    \n",
    "    for i, a in enumerate(alphas):\n",
    "        for repeat in range(n_repeats):\n",
    "            X_sample, y_sample = resample(X, y)\n",
    "            mdl = model(alpha=a)\n",
    "            mdl.fit(X_sample, y_sample)\n",
    "            predictions[i, :, repeat] = mdl.predict(X)\n",
    "    \n",
    "    mean_predictions = np.mean(predictions, axis=2)\n",
    "    biases_squared = np.mean((y - mean_predictions)**2, axis=1)\n",
    "    variances = np.mean(np.var(predictions, axis=2), axis=1)\n",
    "    mses = biases_squared + variances\n",
    "    \n",
    "    return biases_squared, variances, mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_variance_tradeoff(lambdas, bias_squared, variances, mses, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(lambdas, bias_squared, label='Bias²', marker='o', linestyle='-', linewidth=2)\n",
    "    plt.plot(lambdas, variances, label='Variance', marker='s', linestyle='-', linewidth=2)\n",
    "    plt.plot(lambdas, mses, label='MSE', marker='^', linestyle='-', linewidth=3, color='red')\n",
    "    \n",
    "    optimal_idx = np.argmin(mses)\n",
    "    plt.axvline(lambdas[optimal_idx], color='gray', linestyle='--', \n",
    "                label=f'Optimal λ={lambdas[optimal_idx]:.1f}')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Lambda (Regularization Parameter)', fontsize=12)\n",
    "    plt.ylabel('Error', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/konstanty/Projects/UW/ML25/Lab4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m current_dir = os.getcwd()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(current_dir)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m filename = os.getcwd(\u001b[34;43m__file__\u001b[39;49m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(filename)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import random\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UWvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
